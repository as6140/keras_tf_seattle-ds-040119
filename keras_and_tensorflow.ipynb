{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks with keras and tensorflow\n",
    "\n",
    "N.B. You will need to pip install keras and tensorflow!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this lesson we'll use sklearn's built-in breast cancer dataset. The next cell loads the data and prints the data description."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".. _breast_cancer_dataset:\n",
      "\n",
      "Breast cancer wisconsin (diagnostic) dataset\n",
      "--------------------------------------------\n",
      "\n",
      "**Data Set Characteristics:**\n",
      "\n",
      "    :Number of Instances: 569\n",
      "\n",
      "    :Number of Attributes: 30 numeric, predictive attributes and the class\n",
      "\n",
      "    :Attribute Information:\n",
      "        - radius (mean of distances from center to points on the perimeter)\n",
      "        - texture (standard deviation of gray-scale values)\n",
      "        - perimeter\n",
      "        - area\n",
      "        - smoothness (local variation in radius lengths)\n",
      "        - compactness (perimeter^2 / area - 1.0)\n",
      "        - concavity (severity of concave portions of the contour)\n",
      "        - concave points (number of concave portions of the contour)\n",
      "        - symmetry \n",
      "        - fractal dimension (\"coastline approximation\" - 1)\n",
      "\n",
      "        The mean, standard error, and \"worst\" or largest (mean of the three\n",
      "        largest values) of these features were computed for each image,\n",
      "        resulting in 30 features.  For instance, field 3 is Mean Radius, field\n",
      "        13 is Radius SE, field 23 is Worst Radius.\n",
      "\n",
      "        - class:\n",
      "                - WDBC-Malignant\n",
      "                - WDBC-Benign\n",
      "\n",
      "    :Summary Statistics:\n",
      "\n",
      "    ===================================== ====== ======\n",
      "                                           Min    Max\n",
      "    ===================================== ====== ======\n",
      "    radius (mean):                        6.981  28.11\n",
      "    texture (mean):                       9.71   39.28\n",
      "    perimeter (mean):                     43.79  188.5\n",
      "    area (mean):                          143.5  2501.0\n",
      "    smoothness (mean):                    0.053  0.163\n",
      "    compactness (mean):                   0.019  0.345\n",
      "    concavity (mean):                     0.0    0.427\n",
      "    concave points (mean):                0.0    0.201\n",
      "    symmetry (mean):                      0.106  0.304\n",
      "    fractal dimension (mean):             0.05   0.097\n",
      "    radius (standard error):              0.112  2.873\n",
      "    texture (standard error):             0.36   4.885\n",
      "    perimeter (standard error):           0.757  21.98\n",
      "    area (standard error):                6.802  542.2\n",
      "    smoothness (standard error):          0.002  0.031\n",
      "    compactness (standard error):         0.002  0.135\n",
      "    concavity (standard error):           0.0    0.396\n",
      "    concave points (standard error):      0.0    0.053\n",
      "    symmetry (standard error):            0.008  0.079\n",
      "    fractal dimension (standard error):   0.001  0.03\n",
      "    radius (worst):                       7.93   36.04\n",
      "    texture (worst):                      12.02  49.54\n",
      "    perimeter (worst):                    50.41  251.2\n",
      "    area (worst):                         185.2  4254.0\n",
      "    smoothness (worst):                   0.071  0.223\n",
      "    compactness (worst):                  0.027  1.058\n",
      "    concavity (worst):                    0.0    1.252\n",
      "    concave points (worst):               0.0    0.291\n",
      "    symmetry (worst):                     0.156  0.664\n",
      "    fractal dimension (worst):            0.055  0.208\n",
      "    ===================================== ====== ======\n",
      "\n",
      "    :Missing Attribute Values: None\n",
      "\n",
      "    :Class Distribution: 212 - Malignant, 357 - Benign\n",
      "\n",
      "    :Creator:  Dr. William H. Wolberg, W. Nick Street, Olvi L. Mangasarian\n",
      "\n",
      "    :Donor: Nick Street\n",
      "\n",
      "    :Date: November, 1995\n",
      "\n",
      "This is a copy of UCI ML Breast Cancer Wisconsin (Diagnostic) datasets.\n",
      "https://goo.gl/U2Uwz2\n",
      "\n",
      "Features are computed from a digitized image of a fine needle\n",
      "aspirate (FNA) of a breast mass.  They describe\n",
      "characteristics of the cell nuclei present in the image.\n",
      "\n",
      "Separating plane described above was obtained using\n",
      "Multisurface Method-Tree (MSM-T) [K. P. Bennett, \"Decision Tree\n",
      "Construction Via Linear Programming.\" Proceedings of the 4th\n",
      "Midwest Artificial Intelligence and Cognitive Science Society,\n",
      "pp. 97-101, 1992], a classification method which uses linear\n",
      "programming to construct a decision tree.  Relevant features\n",
      "were selected using an exhaustive search in the space of 1-4\n",
      "features and 1-3 separating planes.\n",
      "\n",
      "The actual linear program used to obtain the separating plane\n",
      "in the 3-dimensional space is that described in:\n",
      "[K. P. Bennett and O. L. Mangasarian: \"Robust Linear\n",
      "Programming Discrimination of Two Linearly Inseparable Sets\",\n",
      "Optimization Methods and Software 1, 1992, 23-34].\n",
      "\n",
      "This database is also available through the UW CS ftp server:\n",
      "\n",
      "ftp ftp.cs.wisc.edu\n",
      "cd math-prog/cpo-dataset/machine-learn/WDBC/\n",
      "\n",
      ".. topic:: References\n",
      "\n",
      "   - W.N. Street, W.H. Wolberg and O.L. Mangasarian. Nuclear feature extraction \n",
      "     for breast tumor diagnosis. IS&T/SPIE 1993 International Symposium on \n",
      "     Electronic Imaging: Science and Technology, volume 1905, pages 861-870,\n",
      "     San Jose, CA, 1993.\n",
      "   - O.L. Mangasarian, W.N. Street and W.H. Wolberg. Breast cancer diagnosis and \n",
      "     prognosis via linear programming. Operations Research, 43(4), pages 570-577, \n",
      "     July-August 1995.\n",
      "   - W.H. Wolberg, W.N. Street, and O.L. Mangasarian. Machine learning techniques\n",
      "     to diagnose breast cancer from fine-needle aspirates. Cancer Letters 77 (1994) \n",
      "     163-171.\n"
     ]
    }
   ],
   "source": [
    "data = load_breast_cancer()\n",
    "print(data.DESCR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(data.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StandardScaler(copy=True, with_mean=True, with_std=True)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Splitting our data and initializing a Scaler\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(data.data, data.target,random_state=42)\n",
    "\n",
    "ss = StandardScaler()\n",
    "ss.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(426, 30)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Transforming our data\n",
    "\n",
    "X_train_s = ss.transform(X_train)\n",
    "X_test_s = ss.transform(X_test)\n",
    "\n",
    "X_train_s.shape #30 predictors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-3.49138486e-01],\n",
       "       [-2.04686647e-01],\n",
       "       [-3.29311763e-01],\n",
       "       [ 1.02740257e+00],\n",
       "       [ 1.82896865e+00],\n",
       "       [ 3.27802485e-01],\n",
       "       [-9.70558648e-02],\n",
       "       [ 2.76819483e-01],\n",
       "       [-5.04919881e-01],\n",
       "       [-1.67866188e+00],\n",
       "       [-1.90524702e-01],\n",
       "       [-7.93823558e-01],\n",
       "       [ 2.30764239e+00],\n",
       "       [ 1.49474675e+00],\n",
       "       [-1.80215404e+00],\n",
       "       [ 2.08842147e-01],\n",
       "       [-8.47638949e-01],\n",
       "       [-1.24700580e+00],\n",
       "       [-1.27929503e+00],\n",
       "       [-1.14503979e+00],\n",
       "       [-4.42607323e-01],\n",
       "       [ 2.85316650e-01],\n",
       "       [-1.03174423e+00],\n",
       "       [-3.80294765e-01],\n",
       "       [ 1.74853479e-01],\n",
       "       [-1.19488984e+00],\n",
       "       [-8.57263088e-02],\n",
       "       [-3.49138486e-01],\n",
       "       [-1.08385421e-01],\n",
       "       [-2.13183814e-01],\n",
       "       [ 1.60691534e-01],\n",
       "       [-3.43473708e-01],\n",
       "       [-1.42374089e-01],\n",
       "       [ 9.14107008e-01],\n",
       "       [ 2.48495593e-01],\n",
       "       [-6.57868886e-01],\n",
       "       [ 1.15373310e-01],\n",
       "       [-1.16882588e-01],\n",
       "       [-7.82494002e-01],\n",
       "       [-1.52854526e+00],\n",
       "       [ 5.62890772e-01],\n",
       "       [ 1.23699935e+00],\n",
       "       [-1.16882588e-01],\n",
       "       [ 9.83789761e-02],\n",
       "       [-1.65033201e-01],\n",
       "       [ 3.78785487e-01],\n",
       "       [-3.60468042e-01],\n",
       "       [ 1.44093136e+00],\n",
       "       [-4.68098824e-01],\n",
       "       [-4.70931213e-01],\n",
       "       [-9.24113452e-01],\n",
       "       [-7.79661613e-01],\n",
       "       [ 1.32763580e+00],\n",
       "       [-1.79195146e-01],\n",
       "       [-1.29034135e+00],\n",
       "       [ 8.54626839e-01],\n",
       "       [-8.67465672e-01],\n",
       "       [ 1.38032422e-01],\n",
       "       [-1.82169753e+00],\n",
       "       [ 1.40127792e+00],\n",
       "       [-1.22547366e-01],\n",
       "       [ 1.42676942e+00],\n",
       "       [ 1.60804231e+00],\n",
       "       [-7.54170112e-01],\n",
       "       [-5.84226772e-01],\n",
       "       [-2.75496372e-01],\n",
       "       [-1.45206478e-01],\n",
       "       [-1.09405679e+00],\n",
       "       [-1.35661925e+00],\n",
       "       [-8.67465672e-01],\n",
       "       [-9.80761232e-01],\n",
       "       [-6.46539330e-01],\n",
       "       [-1.02324707e+00],\n",
       "       [-4.11451044e-01],\n",
       "       [-8.61800894e-01],\n",
       "       [-8.73130450e-01],\n",
       "       [-4.79428380e-01],\n",
       "       [ 1.66356312e-01],\n",
       "       [ 2.11674536e-01],\n",
       "       [ 7.15839778e-01],\n",
       "       [ 5.68555550e-01],\n",
       "       [-3.54803264e-01],\n",
       "       [-1.31923172e+00],\n",
       "       [ 2.16035817e+00],\n",
       "       [-2.72663983e-01],\n",
       "       [-9.09951507e-01],\n",
       "       [-5.30411381e-01],\n",
       "       [-3.88791932e-01],\n",
       "       [ 1.29535255e-01],\n",
       "       [-5.24746603e-01],\n",
       "       [-5.55902882e-01],\n",
       "       [ 4.17311961e-02],\n",
       "       [ 2.65489927e-01],\n",
       "       [ 3.70967495e+00],\n",
       "       [-3.20814596e-01],\n",
       "       [-2.41507704e-01],\n",
       "       [ 1.99324722e+00],\n",
       "       [-7.15643638e-02],\n",
       "       [-7.15643638e-02],\n",
       "       [ 1.71000832e+00],\n",
       "       [-1.50871256e-01],\n",
       "       [-4.73763602e-01],\n",
       "       [-1.46509975e+00],\n",
       "       [ 1.28231758e+00],\n",
       "       [-1.33876922e-01],\n",
       "       [ 4.21271322e-01],\n",
       "       [-7.51337723e-01],\n",
       "       [ 8.42170311e-02],\n",
       "       [ 1.57859145e-01],\n",
       "       [-5.87059161e-01],\n",
       "       [-1.25748564e+00],\n",
       "       [-8.92957173e-01],\n",
       "       [-7.65499668e-01],\n",
       "       [-4.48272101e-01],\n",
       "       [ 8.31967727e-01],\n",
       "       [ 6.15579191e-02],\n",
       "       [ 1.29535255e-01],\n",
       "       [-6.69198442e-01],\n",
       "       [-3.66132820e-01],\n",
       "       [-3.09485040e-01],\n",
       "       [ 7.69655169e-01],\n",
       "       [ 1.80064476e+00],\n",
       "       [-2.58502038e-01],\n",
       "       [-1.54412340e+00],\n",
       "       [-6.38042163e-01],\n",
       "       [-3.85959543e-01],\n",
       "       [ 2.08842147e-01],\n",
       "       [-4.17115822e-01],\n",
       "       [-7.96655947e-01],\n",
       "       [-6.18215440e-01],\n",
       "       [ 9.48095675e-01],\n",
       "       [ 2.37166037e-01],\n",
       "       [-7.14516666e-01],\n",
       "       [-5.04919881e-01],\n",
       "       [-1.24389017e+00],\n",
       "       [-8.07985503e-01],\n",
       "       [ 1.69018159e+00],\n",
       "       [-2.90785288e-02],\n",
       "       [-1.02324707e+00],\n",
       "       [-1.20282053e+00],\n",
       "       [-7.23013833e-01],\n",
       "       [-5.13417048e-01],\n",
       "       [-1.31044533e-01],\n",
       "       [-1.07706246e+00],\n",
       "       [ 1.08405035e+00],\n",
       "       [-5.41740937e-01],\n",
       "       [-5.41740937e-01],\n",
       "       [-3.54803264e-01],\n",
       "       [ 7.27169334e-01],\n",
       "       [-5.50238104e-01],\n",
       "       [ 6.30868108e-01],\n",
       "       [ 6.73353943e-01],\n",
       "       [-5.89891550e-01],\n",
       "       [-1.15353696e+00],\n",
       "       [ 3.60664181e-02],\n",
       "       [-8.05153114e-01],\n",
       "       [ 8.99945063e-01],\n",
       "       [ 1.83180104e+00],\n",
       "       [-1.55913507e+00],\n",
       "       [-4.22780600e-01],\n",
       "       [ 6.08208996e-01],\n",
       "       [-4.48272101e-01],\n",
       "       [ 8.97112674e-01],\n",
       "       [ 1.10670946e+00],\n",
       "       [-6.69198442e-01],\n",
       "       [-3.17982207e-01],\n",
       "       [-9.97755566e-01],\n",
       "       [-7.08851888e-01],\n",
       "       [ 2.45663204e-01],\n",
       "       [-6.32377385e-01],\n",
       "       [-1.45093781e+00],\n",
       "       [ 1.43243419e+00],\n",
       "       [-1.43904177e+00],\n",
       "       [-6.57868886e-01],\n",
       "       [-6.30671968e-02],\n",
       "       [ 2.34333648e-01],\n",
       "       [-1.87692313e-01],\n",
       "       [ 3.28764899e+00],\n",
       "       [ 1.06876143e-01],\n",
       "       [ 2.97608620e+00],\n",
       "       [-5.50238104e-01],\n",
       "       [-3.46306097e-01],\n",
       "       [ 9.45263287e-01],\n",
       "       [ 1.50607631e+00],\n",
       "       [-1.22802879e+00],\n",
       "       [-5.44573326e-01],\n",
       "       [ 1.79781237e+00],\n",
       "       [-6.43706941e-01],\n",
       "       [ 1.71567309e+00],\n",
       "       [ 2.86845542e+00],\n",
       "       [-7.68332057e-01],\n",
       "       [ 2.59654607e+00],\n",
       "       [ 1.76099132e+00],\n",
       "       [-2.61334427e-01],\n",
       "       [ 5.79885106e-01],\n",
       "       [ 1.78931521e+00],\n",
       "       [ 1.33046819e+00],\n",
       "       [ 2.14902861e+00],\n",
       "       [-4.73763602e-01],\n",
       "       [-6.15383051e-01],\n",
       "       [-7.72291418e-02],\n",
       "       [ 3.02310984e-01],\n",
       "       [ 4.60924768e-01],\n",
       "       [-8.67465672e-01],\n",
       "       [ 1.16052485e+00],\n",
       "       [ 1.53156781e+00],\n",
       "       [ 1.60520992e+00],\n",
       "       [ 2.62657538e-01],\n",
       "       [-8.44806560e-01],\n",
       "       [-3.34976541e-01],\n",
       "       [-2.50004871e-01],\n",
       "       [-3.03820262e-01],\n",
       "       [-7.72291418e-02],\n",
       "       [-2.33010537e-01],\n",
       "       [-3.97289099e-01],\n",
       "       [-6.01221106e-01],\n",
       "       [ 1.54856214e+00],\n",
       "       [ 1.84596299e+00],\n",
       "       [ 3.88988071e-02],\n",
       "       [-3.71797598e-01],\n",
       "       [ 8.37632505e-01],\n",
       "       [-1.33622605e+00],\n",
       "       [-9.01454340e-01],\n",
       "       [-1.56508308e+00],\n",
       "       [-1.67214739e+00],\n",
       "       [-1.00625273e+00],\n",
       "       [ 1.80914193e+00],\n",
       "       [-2.61334427e-01],\n",
       "       [-4.90757936e-01],\n",
       "       [ 2.59088129e+00],\n",
       "       [ 1.16052485e+00],\n",
       "       [-6.83360387e-01],\n",
       "       [ 1.12087140e+00],\n",
       "       [-2.34137508e-02],\n",
       "       [ 4.35433267e-01],\n",
       "       [-8.22147448e-01],\n",
       "       [-6.21047829e-01],\n",
       "       [-7.68332057e-01],\n",
       "       [-3.97289099e-01],\n",
       "       [-1.00625273e+00],\n",
       "       [-2.41507704e-01],\n",
       "       [ 1.10104468e+00],\n",
       "       [-1.33849196e+00],\n",
       "       [ 1.80518257e-01],\n",
       "       [ 1.38145119e+00],\n",
       "       [-7.40008167e-01],\n",
       "       [-4.36942545e-01],\n",
       "       [-1.21103446e+00],\n",
       "       [-1.25946831e+00],\n",
       "       [-4.99255103e-01],\n",
       "       [-7.06019499e-01],\n",
       "       [ 1.07272079e+00],\n",
       "       [ 4.49595212e-01],\n",
       "       [ 1.91847813e-01],\n",
       "       [-2.69831594e-01],\n",
       "       [ 2.39998426e-01],\n",
       "       [ 1.86183035e-01],\n",
       "       [ 5.99711829e-01],\n",
       "       [ 1.22850219e+00],\n",
       "       [-7.99488336e-01],\n",
       "       [ 1.18601635e+00],\n",
       "       [ 2.25836481e-01],\n",
       "       [-9.12783896e-01],\n",
       "       [ 1.42960181e+00],\n",
       "       [ 7.41331279e-01],\n",
       "       [ 3.33467263e-01],\n",
       "       [-5.10584659e-01],\n",
       "       [-7.62667279e-01],\n",
       "       [-6.91857554e-01],\n",
       "       [-8.39141782e-01],\n",
       "       [ 1.15373310e-01],\n",
       "       [-1.59368423e-01],\n",
       "       [-7.88158780e-01],\n",
       "       [ 1.73833221e+00],\n",
       "       [-1.36709311e-01],\n",
       "       [-1.36709311e-01],\n",
       "       [ 1.32367644e-01],\n",
       "       [ 4.60924768e-01],\n",
       "       [-1.26173422e+00],\n",
       "       [-3.15149818e-01],\n",
       "       [ 6.02544218e-01],\n",
       "       [-8.98621951e-01],\n",
       "       [-1.57244729e+00],\n",
       "       [ 3.78785487e-01],\n",
       "       [-1.20282053e+00],\n",
       "       [ 3.87282654e-01],\n",
       "       [ 1.49474675e+00],\n",
       "       [-1.48549295e+00],\n",
       "       [-3.74629987e-01],\n",
       "       [ 1.50890870e+00],\n",
       "       [-5.38908548e-01],\n",
       "       [ 1.74682937e+00],\n",
       "       [ 1.53723259e+00],\n",
       "       [-8.95789562e-01],\n",
       "       [ 2.20171703e-01],\n",
       "       [-1.45206478e-01],\n",
       "       [-1.04307379e+00],\n",
       "       [-3.51970875e-01],\n",
       "       [ 9.33933731e-01],\n",
       "       [-1.32744565e+00],\n",
       "       [-1.96189480e-01],\n",
       "       [-7.14516666e-01],\n",
       "       [-3.32144152e-01],\n",
       "       [-7.43967528e-02],\n",
       "       [-1.56479984e+00],\n",
       "       [ 5.71387939e-01],\n",
       "       [-2.69831594e-01],\n",
       "       [ 8.20638171e-01],\n",
       "       [ 3.90115043e-01],\n",
       "       [-1.50871256e-01],\n",
       "       [-5.92723939e-01],\n",
       "       [-5.36076159e-01],\n",
       "       [ 1.22566980e+00],\n",
       "       [ 1.27098802e+00],\n",
       "       [-7.45672945e-01],\n",
       "       [ 8.29135338e-01],\n",
       "       [ 1.75249415e+00],\n",
       "       [-1.31044533e-01],\n",
       "       [-8.10817892e-01],\n",
       "       [-4.02953877e-01],\n",
       "       [ 1.54856214e+00],\n",
       "       [-1.52627935e+00],\n",
       "       [ 1.18601635e+00],\n",
       "       [ 2.34333648e-01],\n",
       "       [ 3.60664181e-02],\n",
       "       [-3.83127154e-01],\n",
       "       [ 2.76819483e-01],\n",
       "       [-3.83127154e-01],\n",
       "       [-3.17982207e-01],\n",
       "       [-3.58702785e-03],\n",
       "       [-6.83360387e-01],\n",
       "       [-4.08618655e-01],\n",
       "       [ 1.53156781e+00],\n",
       "       [-1.11217810e-01],\n",
       "       [ 1.00757584e+00],\n",
       "       [-1.93357091e-01],\n",
       "       [-8.61800894e-01],\n",
       "       [-6.83360387e-01],\n",
       "       [-4.70931213e-01],\n",
       "       [-6.32377385e-01],\n",
       "       [-5.24746603e-01],\n",
       "       [-7.72291418e-02],\n",
       "       [-6.18215440e-01],\n",
       "       [-8.10817892e-01],\n",
       "       [ 1.31630625e+00],\n",
       "       [ 2.57388696e+00],\n",
       "       [ 8.17805782e-01],\n",
       "       [ 3.96175757e+00],\n",
       "       [-9.98882538e-02],\n",
       "       [-6.89025165e-01],\n",
       "       [-7.42840556e-01],\n",
       "       [-2.05813618e-02],\n",
       "       [ 3.76632273e+00],\n",
       "       [-2.10351425e-01],\n",
       "       [ 7.01677833e-01],\n",
       "       [-1.02891185e+00],\n",
       "       [-6.97522332e-01],\n",
       "       [-9.66599287e-01],\n",
       "       [-1.19714977e-01],\n",
       "       [-6.04053495e-01],\n",
       "       [ 9.67922398e-01],\n",
       "       [ 1.43526658e+00],\n",
       "       [-2.38675315e-01],\n",
       "       [-1.11105113e+00],\n",
       "       [-6.21047829e-01],\n",
       "       [ 2.75692511e-02],\n",
       "       [ 2.19044731e-02],\n",
       "       [-3.03820262e-01],\n",
       "       [ 3.98612210e-01],\n",
       "       [-2.86825928e-01],\n",
       "       [ 1.53723259e+00],\n",
       "       [ 1.82613626e+00],\n",
       "       [-5.81394383e-01],\n",
       "       [-7.57002501e-01],\n",
       "       [ 2.82484261e-01],\n",
       "       [ 5.68555550e-01],\n",
       "       [ 6.16706163e-01],\n",
       "       [-9.41107786e-01],\n",
       "       [-4.25612989e-01],\n",
       "       [ 1.72133787e+00],\n",
       "       [ 3.53293986e-01],\n",
       "       [ 2.65885863e+00],\n",
       "       [-1.09405679e+00],\n",
       "       [-4.82260769e-01],\n",
       "       [ 4.89248658e-01],\n",
       "       [-4.62434046e-01],\n",
       "       [-6.87319748e-02],\n",
       "       [-1.24077454e+00],\n",
       "       [-9.41107786e-01],\n",
       "       [ 1.92526988e+00],\n",
       "       [ 4.55259990e-01],\n",
       "       [-3.51970875e-01],\n",
       "       [-1.37502978e+00],\n",
       "       [-1.05553032e-01],\n",
       "       [-5.16249437e-01],\n",
       "       [-3.66132820e-01],\n",
       "       [-6.74863220e-01],\n",
       "       [-3.03820262e-01],\n",
       "       [-9.18448674e-01],\n",
       "       [ 1.58821559e+00],\n",
       "       [-1.30818540e+00],\n",
       "       [-7.31511000e-01],\n",
       "       [-1.23709244e+00],\n",
       "       [-6.72030831e-01],\n",
       "       [-7.90991169e-01],\n",
       "       [-3.83127154e-01],\n",
       "       [ 1.35200033e-01],\n",
       "       [ 2.45663204e-01],\n",
       "       [ 1.05572646e+00],\n",
       "       [ 1.57405364e+00],\n",
       "       [-1.76362757e-01],\n",
       "       [-5.47405715e-01],\n",
       "       [ 8.42170311e-02],\n",
       "       [ 2.05272738e+00],\n",
       "       [ 1.38711597e+00],\n",
       "       [-3.17982207e-01],\n",
       "       [ 5.40231660e-01],\n",
       "       [ 1.90720841e-02],\n",
       "       [-2.78328761e-01],\n",
       "       [ 1.28514997e+00],\n",
       "       [-2.95323095e-01],\n",
       "       [-1.48266056e+00],\n",
       "       [-7.03187110e-01],\n",
       "       [ 4.73959741e-02],\n",
       "       [-4.04080848e-02],\n",
       "       [-5.50238104e-01]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_s[:,:1] # 1st column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.34913849, -1.43851335, -0.41172595, -0.39047943, -1.86366229,\n",
       "        -1.26860704, -0.82617052, -0.95286585, -1.72936805, -0.9415409 ,\n",
       "        -0.86971355, -1.35865347, -0.83481506, -0.57230673, -0.74586846,\n",
       "        -0.65398319, -0.52583524, -0.94677147, -0.53781728, -0.63449458,\n",
       "        -0.54268486, -1.65565452, -0.58986401, -0.52555985, -1.51066925,\n",
       "        -0.89149994, -0.75021715, -0.91671059, -0.92508585, -0.80841115]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_s[:1,:] #1st row"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constructing a Neural Network in Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Importing model and layer types\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "#import our optimizer\n",
    "\n",
    "from keras.optimizers import Adam #we choose Adam as our gradient descent optimization algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constructing and compiling our model\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "inputs = X_train_s.shape[1]\n",
    "\n",
    "hiddens = inputs #so that we have same # of inputs into hidden layer as # of neurons\n",
    "\n",
    "model.add(Dense(hiddens, input_dim=inputs, activation='relu'))\n",
    "model.add(Dense(1,activation='sigmoid')) #adding a layer with 1 neuron in it,\n",
    "adam = Adam()\n",
    "\n",
    "model.compile(optimizer=adam, loss='mean_squared_error') #bundling layers, optimizer, and loss function together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 426 samples, validate on 143 samples\n",
      "Epoch 1/100\n",
      "426/426 [==============================] - 0s 86us/step - loss: 0.0057 - val_loss: 0.0174\n",
      "Epoch 2/100\n",
      "426/426 [==============================] - 0s 61us/step - loss: 0.0056 - val_loss: 0.0174\n",
      "Epoch 3/100\n",
      "426/426 [==============================] - 0s 56us/step - loss: 0.0056 - val_loss: 0.0176\n",
      "Epoch 4/100\n",
      "426/426 [==============================] - 0s 94us/step - loss: 0.0055 - val_loss: 0.0175\n",
      "Epoch 5/100\n",
      "426/426 [==============================] - 0s 118us/step - loss: 0.0055 - val_loss: 0.0175\n",
      "Epoch 6/100\n",
      "426/426 [==============================] - 0s 100us/step - loss: 0.0054 - val_loss: 0.0175\n",
      "Epoch 7/100\n",
      "426/426 [==============================] - 0s 95us/step - loss: 0.0054 - val_loss: 0.0178\n",
      "Epoch 8/100\n",
      "426/426 [==============================] - 0s 86us/step - loss: 0.0053 - val_loss: 0.0178\n",
      "Epoch 9/100\n",
      "426/426 [==============================] - 0s 109us/step - loss: 0.0053 - val_loss: 0.0178\n",
      "Epoch 10/100\n",
      "426/426 [==============================] - 0s 103us/step - loss: 0.0053 - val_loss: 0.0182\n",
      "Epoch 11/100\n",
      "426/426 [==============================] - 0s 93us/step - loss: 0.0052 - val_loss: 0.0182\n",
      "Epoch 12/100\n",
      "426/426 [==============================] - 0s 74us/step - loss: 0.0051 - val_loss: 0.0179\n",
      "Epoch 13/100\n",
      "426/426 [==============================] - 0s 101us/step - loss: 0.0051 - val_loss: 0.0179\n",
      "Epoch 14/100\n",
      "426/426 [==============================] - 0s 93us/step - loss: 0.0051 - val_loss: 0.0180\n",
      "Epoch 15/100\n",
      "426/426 [==============================] - 0s 78us/step - loss: 0.0050 - val_loss: 0.0179\n",
      "Epoch 16/100\n",
      "426/426 [==============================] - 0s 101us/step - loss: 0.0050 - val_loss: 0.0180\n",
      "Epoch 17/100\n",
      "426/426 [==============================] - 0s 92us/step - loss: 0.0049 - val_loss: 0.0177\n",
      "Epoch 18/100\n",
      "426/426 [==============================] - 0s 81us/step - loss: 0.0049 - val_loss: 0.0177\n",
      "Epoch 19/100\n",
      "426/426 [==============================] - 0s 100us/step - loss: 0.0048 - val_loss: 0.0177\n",
      "Epoch 20/100\n",
      "426/426 [==============================] - 0s 87us/step - loss: 0.0048 - val_loss: 0.0180\n",
      "Epoch 21/100\n",
      "426/426 [==============================] - 0s 81us/step - loss: 0.0048 - val_loss: 0.0180\n",
      "Epoch 22/100\n",
      "426/426 [==============================] - 0s 89us/step - loss: 0.0047 - val_loss: 0.0181\n",
      "Epoch 23/100\n",
      "426/426 [==============================] - 0s 87us/step - loss: 0.0047 - val_loss: 0.0180\n",
      "Epoch 24/100\n",
      "426/426 [==============================] - 0s 102us/step - loss: 0.0047 - val_loss: 0.0179\n",
      "Epoch 25/100\n",
      "426/426 [==============================] - 0s 98us/step - loss: 0.0046 - val_loss: 0.0189\n",
      "Epoch 26/100\n",
      "426/426 [==============================] - 0s 82us/step - loss: 0.0046 - val_loss: 0.0189\n",
      "Epoch 27/100\n",
      "426/426 [==============================] - 0s 82us/step - loss: 0.0045 - val_loss: 0.0185\n",
      "Epoch 28/100\n",
      "426/426 [==============================] - 0s 89us/step - loss: 0.0045 - val_loss: 0.0180\n",
      "Epoch 29/100\n",
      "426/426 [==============================] - 0s 92us/step - loss: 0.0044 - val_loss: 0.0182\n",
      "Epoch 30/100\n",
      "426/426 [==============================] - 0s 86us/step - loss: 0.0044 - val_loss: 0.0182\n",
      "Epoch 31/100\n",
      "426/426 [==============================] - 0s 70us/step - loss: 0.0044 - val_loss: 0.0182\n",
      "Epoch 32/100\n",
      "426/426 [==============================] - 0s 80us/step - loss: 0.0043 - val_loss: 0.0184\n",
      "Epoch 33/100\n",
      "426/426 [==============================] - 0s 72us/step - loss: 0.0043 - val_loss: 0.0183\n",
      "Epoch 34/100\n",
      "426/426 [==============================] - 0s 95us/step - loss: 0.0043 - val_loss: 0.0185\n",
      "Epoch 35/100\n",
      "426/426 [==============================] - 0s 90us/step - loss: 0.0042 - val_loss: 0.0184\n",
      "Epoch 36/100\n",
      "426/426 [==============================] - 0s 68us/step - loss: 0.0042 - val_loss: 0.0184\n",
      "Epoch 37/100\n",
      "426/426 [==============================] - 0s 78us/step - loss: 0.0042 - val_loss: 0.0186\n",
      "Epoch 38/100\n",
      "426/426 [==============================] - 0s 85us/step - loss: 0.0042 - val_loss: 0.0187\n",
      "Epoch 39/100\n",
      "426/426 [==============================] - 0s 63us/step - loss: 0.0042 - val_loss: 0.0183\n",
      "Epoch 40/100\n",
      "426/426 [==============================] - 0s 73us/step - loss: 0.0041 - val_loss: 0.0186\n",
      "Epoch 41/100\n",
      "426/426 [==============================] - 0s 74us/step - loss: 0.0041 - val_loss: 0.0182\n",
      "Epoch 42/100\n",
      "426/426 [==============================] - 0s 65us/step - loss: 0.0041 - val_loss: 0.0184\n",
      "Epoch 43/100\n",
      "426/426 [==============================] - 0s 78us/step - loss: 0.0041 - val_loss: 0.0184\n",
      "Epoch 44/100\n",
      "426/426 [==============================] - 0s 68us/step - loss: 0.0041 - val_loss: 0.0200\n",
      "Epoch 45/100\n",
      "426/426 [==============================] - 0s 67us/step - loss: 0.0042 - val_loss: 0.0198\n",
      "Epoch 46/100\n",
      "426/426 [==============================] - 0s 75us/step - loss: 0.0040 - val_loss: 0.0193\n",
      "Epoch 47/100\n",
      "426/426 [==============================] - 0s 74us/step - loss: 0.0039 - val_loss: 0.0192\n",
      "Epoch 48/100\n",
      "426/426 [==============================] - 0s 76us/step - loss: 0.0039 - val_loss: 0.0187\n",
      "Epoch 49/100\n",
      "426/426 [==============================] - 0s 73us/step - loss: 0.0039 - val_loss: 0.0189\n",
      "Epoch 50/100\n",
      "426/426 [==============================] - 0s 83us/step - loss: 0.0038 - val_loss: 0.0188\n",
      "Epoch 51/100\n",
      "426/426 [==============================] - 0s 80us/step - loss: 0.0038 - val_loss: 0.0187\n",
      "Epoch 52/100\n",
      "426/426 [==============================] - 0s 78us/step - loss: 0.0038 - val_loss: 0.0186\n",
      "Epoch 53/100\n",
      "426/426 [==============================] - 0s 90us/step - loss: 0.0038 - val_loss: 0.0188\n",
      "Epoch 54/100\n",
      "426/426 [==============================] - 0s 73us/step - loss: 0.0038 - val_loss: 0.0188\n",
      "Epoch 55/100\n",
      "426/426 [==============================] - 0s 77us/step - loss: 0.0037 - val_loss: 0.0189\n",
      "Epoch 56/100\n",
      "426/426 [==============================] - 0s 78us/step - loss: 0.0037 - val_loss: 0.0191\n",
      "Epoch 57/100\n",
      "426/426 [==============================] - 0s 62us/step - loss: 0.0037 - val_loss: 0.0188\n",
      "Epoch 58/100\n",
      "426/426 [==============================] - 0s 73us/step - loss: 0.0037 - val_loss: 0.0190\n",
      "Epoch 59/100\n",
      "426/426 [==============================] - 0s 93us/step - loss: 0.0037 - val_loss: 0.0190\n",
      "Epoch 60/100\n",
      "426/426 [==============================] - 0s 86us/step - loss: 0.0036 - val_loss: 0.0190\n",
      "Epoch 61/100\n",
      "426/426 [==============================] - 0s 77us/step - loss: 0.0036 - val_loss: 0.0191\n",
      "Epoch 62/100\n",
      "426/426 [==============================] - 0s 74us/step - loss: 0.0036 - val_loss: 0.0196\n",
      "Epoch 63/100\n",
      "426/426 [==============================] - 0s 69us/step - loss: 0.0036 - val_loss: 0.0196\n",
      "Epoch 64/100\n",
      "426/426 [==============================] - 0s 71us/step - loss: 0.0036 - val_loss: 0.0192\n",
      "Epoch 65/100\n",
      "426/426 [==============================] - 0s 70us/step - loss: 0.0036 - val_loss: 0.0195\n",
      "Epoch 66/100\n",
      "426/426 [==============================] - 0s 74us/step - loss: 0.0035 - val_loss: 0.0195\n",
      "Epoch 67/100\n",
      "426/426 [==============================] - 0s 75us/step - loss: 0.0035 - val_loss: 0.0193\n",
      "Epoch 68/100\n",
      "426/426 [==============================] - 0s 71us/step - loss: 0.0035 - val_loss: 0.0193\n",
      "Epoch 69/100\n",
      "426/426 [==============================] - 0s 74us/step - loss: 0.0035 - val_loss: 0.0192\n",
      "Epoch 70/100\n",
      "426/426 [==============================] - 0s 83us/step - loss: 0.0035 - val_loss: 0.0194\n",
      "Epoch 71/100\n",
      "426/426 [==============================] - 0s 87us/step - loss: 0.0035 - val_loss: 0.0195\n",
      "Epoch 72/100\n",
      "426/426 [==============================] - 0s 83us/step - loss: 0.0034 - val_loss: 0.0196\n",
      "Epoch 73/100\n",
      "426/426 [==============================] - 0s 77us/step - loss: 0.0034 - val_loss: 0.0195\n",
      "Epoch 74/100\n",
      "426/426 [==============================] - 0s 84us/step - loss: 0.0034 - val_loss: 0.0199\n",
      "Epoch 75/100\n",
      "426/426 [==============================] - 0s 77us/step - loss: 0.0034 - val_loss: 0.0200\n",
      "Epoch 76/100\n",
      "426/426 [==============================] - 0s 73us/step - loss: 0.0034 - val_loss: 0.0200\n",
      "Epoch 77/100\n",
      "426/426 [==============================] - 0s 73us/step - loss: 0.0034 - val_loss: 0.0200\n",
      "Epoch 78/100\n",
      "426/426 [==============================] - 0s 67us/step - loss: 0.0033 - val_loss: 0.0199\n",
      "Epoch 79/100\n",
      "426/426 [==============================] - 0s 70us/step - loss: 0.0033 - val_loss: 0.0198\n",
      "Epoch 80/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "426/426 [==============================] - 0s 63us/step - loss: 0.0033 - val_loss: 0.0197\n",
      "Epoch 81/100\n",
      "426/426 [==============================] - 0s 59us/step - loss: 0.0033 - val_loss: 0.0202\n",
      "Epoch 82/100\n",
      "426/426 [==============================] - 0s 73us/step - loss: 0.0033 - val_loss: 0.0199\n",
      "Epoch 83/100\n",
      "426/426 [==============================] - 0s 73us/step - loss: 0.0033 - val_loss: 0.0200\n",
      "Epoch 84/100\n",
      "426/426 [==============================] - 0s 91us/step - loss: 0.0033 - val_loss: 0.0199\n",
      "Epoch 85/100\n",
      "426/426 [==============================] - 0s 84us/step - loss: 0.0033 - val_loss: 0.0200\n",
      "Epoch 86/100\n",
      "426/426 [==============================] - 0s 78us/step - loss: 0.0033 - val_loss: 0.0200\n",
      "Epoch 87/100\n",
      "426/426 [==============================] - 0s 80us/step - loss: 0.0032 - val_loss: 0.0201\n",
      "Epoch 88/100\n",
      "426/426 [==============================] - 0s 94us/step - loss: 0.0032 - val_loss: 0.0199\n",
      "Epoch 89/100\n",
      "426/426 [==============================] - 0s 85us/step - loss: 0.0032 - val_loss: 0.0199\n",
      "Epoch 90/100\n",
      "426/426 [==============================] - 0s 85us/step - loss: 0.0032 - val_loss: 0.0198\n",
      "Epoch 91/100\n",
      "426/426 [==============================] - 0s 77us/step - loss: 0.0032 - val_loss: 0.0197\n",
      "Epoch 92/100\n",
      "426/426 [==============================] - 0s 80us/step - loss: 0.0032 - val_loss: 0.0201\n",
      "Epoch 93/100\n",
      "426/426 [==============================] - 0s 83us/step - loss: 0.0032 - val_loss: 0.0200\n",
      "Epoch 94/100\n",
      "426/426 [==============================] - 0s 90us/step - loss: 0.0032 - val_loss: 0.0202\n",
      "Epoch 95/100\n",
      "426/426 [==============================] - 0s 79us/step - loss: 0.0031 - val_loss: 0.0201\n",
      "Epoch 96/100\n",
      "426/426 [==============================] - 0s 108us/step - loss: 0.0031 - val_loss: 0.0202\n",
      "Epoch 97/100\n",
      "426/426 [==============================] - 0s 84us/step - loss: 0.0031 - val_loss: 0.0201\n",
      "Epoch 98/100\n",
      "426/426 [==============================] - 0s 77us/step - loss: 0.0031 - val_loss: 0.0204\n",
      "Epoch 99/100\n",
      "426/426 [==============================] - 0s 82us/step - loss: 0.0031 - val_loss: 0.0204\n",
      "Epoch 100/100\n",
      "426/426 [==============================] - 0s 86us/step - loss: 0.0031 - val_loss: 0.0204\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1a1415c438>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fitting our model\n",
    "\n",
    "model.fit(X_train_s, y_train, \n",
    "          validation_data = (X_test_s, y_test), epochs=100) #epochs = # of times we pass data through model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 426 samples, validate on 143 samples\n",
      "Epoch 1/100\n",
      "426/426 [==============================] - 0s 72us/step - loss: 0.0032 - val_loss: 0.0202\n",
      "Epoch 2/100\n",
      "426/426 [==============================] - 0s 62us/step - loss: 0.0031 - val_loss: 0.0206\n",
      "Epoch 3/100\n",
      "426/426 [==============================] - 0s 52us/step - loss: 0.0031 - val_loss: 0.0206\n",
      "Epoch 4/100\n",
      "426/426 [==============================] - 0s 69us/step - loss: 0.0031 - val_loss: 0.0207\n",
      "Epoch 5/100\n",
      "426/426 [==============================] - 0s 106us/step - loss: 0.0031 - val_loss: 0.0209\n",
      "Epoch 6/100\n",
      "426/426 [==============================] - 0s 88us/step - loss: 0.0030 - val_loss: 0.0207\n",
      "Epoch 7/100\n",
      "426/426 [==============================] - 0s 103us/step - loss: 0.0030 - val_loss: 0.0210\n",
      "Epoch 8/100\n",
      "426/426 [==============================] - 0s 98us/step - loss: 0.0030 - val_loss: 0.0212\n",
      "Epoch 9/100\n",
      "426/426 [==============================] - 0s 89us/step - loss: 0.0030 - val_loss: 0.0211\n",
      "Epoch 10/100\n",
      "426/426 [==============================] - 0s 95us/step - loss: 0.0030 - val_loss: 0.0211\n",
      "Epoch 11/100\n",
      "426/426 [==============================] - 0s 87us/step - loss: 0.0030 - val_loss: 0.0209\n",
      "Epoch 12/100\n",
      "426/426 [==============================] - 0s 87us/step - loss: 0.0030 - val_loss: 0.0210\n",
      "Epoch 13/100\n",
      "426/426 [==============================] - 0s 88us/step - loss: 0.0030 - val_loss: 0.0209\n",
      "Epoch 14/100\n",
      "426/426 [==============================] - 0s 90us/step - loss: 0.0030 - val_loss: 0.0210\n",
      "Epoch 15/100\n",
      "426/426 [==============================] - 0s 80us/step - loss: 0.0030 - val_loss: 0.0211\n",
      "Epoch 16/100\n",
      "426/426 [==============================] - 0s 83us/step - loss: 0.0029 - val_loss: 0.0211\n",
      "Epoch 17/100\n",
      "426/426 [==============================] - 0s 87us/step - loss: 0.0029 - val_loss: 0.0212\n",
      "Epoch 18/100\n",
      "426/426 [==============================] - 0s 86us/step - loss: 0.0029 - val_loss: 0.0214\n",
      "Epoch 19/100\n",
      "426/426 [==============================] - 0s 78us/step - loss: 0.0029 - val_loss: 0.0213\n",
      "Epoch 20/100\n",
      "426/426 [==============================] - 0s 89us/step - loss: 0.0029 - val_loss: 0.0213\n",
      "Epoch 21/100\n",
      "426/426 [==============================] - 0s 92us/step - loss: 0.0029 - val_loss: 0.0214\n",
      "Epoch 22/100\n",
      "426/426 [==============================] - 0s 85us/step - loss: 0.0029 - val_loss: 0.0213\n",
      "Epoch 23/100\n",
      "426/426 [==============================] - 0s 74us/step - loss: 0.0029 - val_loss: 0.0223\n",
      "Epoch 24/100\n",
      "426/426 [==============================] - 0s 80us/step - loss: 0.0029 - val_loss: 0.0224\n",
      "Epoch 25/100\n",
      "426/426 [==============================] - 0s 77us/step - loss: 0.0029 - val_loss: 0.0219\n",
      "Epoch 26/100\n",
      "426/426 [==============================] - 0s 85us/step - loss: 0.0029 - val_loss: 0.0217\n",
      "Epoch 27/100\n",
      "426/426 [==============================] - 0s 82us/step - loss: 0.0029 - val_loss: 0.0216\n",
      "Epoch 28/100\n",
      "426/426 [==============================] - 0s 75us/step - loss: 0.0029 - val_loss: 0.0216\n",
      "Epoch 29/100\n",
      "426/426 [==============================] - 0s 94us/step - loss: 0.0029 - val_loss: 0.0217\n",
      "Epoch 30/100\n",
      "426/426 [==============================] - 0s 102us/step - loss: 0.0029 - val_loss: 0.0217\n",
      "Epoch 31/100\n",
      "426/426 [==============================] - 0s 85us/step - loss: 0.0028 - val_loss: 0.0219\n",
      "Epoch 32/100\n",
      "426/426 [==============================] - 0s 65us/step - loss: 0.0028 - val_loss: 0.0218\n",
      "Epoch 33/100\n",
      "426/426 [==============================] - 0s 88us/step - loss: 0.0028 - val_loss: 0.0219\n",
      "Epoch 34/100\n",
      "426/426 [==============================] - 0s 86us/step - loss: 0.0028 - val_loss: 0.0216\n",
      "Epoch 35/100\n",
      "426/426 [==============================] - 0s 116us/step - loss: 0.0028 - val_loss: 0.0218\n",
      "Epoch 36/100\n",
      "426/426 [==============================] - 0s 123us/step - loss: 0.0028 - val_loss: 0.0216\n",
      "Epoch 37/100\n",
      "426/426 [==============================] - 0s 115us/step - loss: 0.0028 - val_loss: 0.0219\n",
      "Epoch 38/100\n",
      "426/426 [==============================] - 0s 73us/step - loss: 0.0028 - val_loss: 0.0218\n",
      "Epoch 39/100\n",
      "426/426 [==============================] - 0s 82us/step - loss: 0.0028 - val_loss: 0.0219\n",
      "Epoch 40/100\n",
      "426/426 [==============================] - 0s 74us/step - loss: 0.0028 - val_loss: 0.0220\n",
      "Epoch 41/100\n",
      "426/426 [==============================] - 0s 79us/step - loss: 0.0028 - val_loss: 0.0223\n",
      "Epoch 42/100\n",
      "426/426 [==============================] - 0s 91us/step - loss: 0.0028 - val_loss: 0.0220\n",
      "Epoch 43/100\n",
      "426/426 [==============================] - 0s 87us/step - loss: 0.0028 - val_loss: 0.0221\n",
      "Epoch 44/100\n",
      "426/426 [==============================] - 0s 74us/step - loss: 0.0028 - val_loss: 0.0222\n",
      "Epoch 45/100\n",
      "426/426 [==============================] - 0s 78us/step - loss: 0.0028 - val_loss: 0.0223\n",
      "Epoch 46/100\n",
      "426/426 [==============================] - 0s 80us/step - loss: 0.0028 - val_loss: 0.0223\n",
      "Epoch 47/100\n",
      "426/426 [==============================] - 0s 75us/step - loss: 0.0027 - val_loss: 0.0224\n",
      "Epoch 48/100\n",
      "426/426 [==============================] - 0s 99us/step - loss: 0.0027 - val_loss: 0.0223\n",
      "Epoch 49/100\n",
      "426/426 [==============================] - 0s 91us/step - loss: 0.0027 - val_loss: 0.0225\n",
      "Epoch 50/100\n",
      "426/426 [==============================] - 0s 102us/step - loss: 0.0027 - val_loss: 0.0223\n",
      "Epoch 51/100\n",
      "426/426 [==============================] - 0s 101us/step - loss: 0.0027 - val_loss: 0.0224\n",
      "Epoch 52/100\n",
      "426/426 [==============================] - 0s 83us/step - loss: 0.0027 - val_loss: 0.0225\n",
      "Epoch 53/100\n",
      "426/426 [==============================] - 0s 78us/step - loss: 0.0027 - val_loss: 0.0223\n",
      "Epoch 54/100\n",
      "426/426 [==============================] - 0s 90us/step - loss: 0.0027 - val_loss: 0.0225\n",
      "Epoch 55/100\n",
      "426/426 [==============================] - 0s 82us/step - loss: 0.0027 - val_loss: 0.0226\n",
      "Epoch 56/100\n",
      "426/426 [==============================] - 0s 105us/step - loss: 0.0027 - val_loss: 0.0228\n",
      "Epoch 57/100\n",
      "426/426 [==============================] - 0s 65us/step - loss: 0.0027 - val_loss: 0.0229\n",
      "Epoch 58/100\n",
      "426/426 [==============================] - 0s 61us/step - loss: 0.0027 - val_loss: 0.0229\n",
      "Epoch 59/100\n",
      "426/426 [==============================] - 0s 86us/step - loss: 0.0027 - val_loss: 0.0229\n",
      "Epoch 60/100\n",
      "426/426 [==============================] - 0s 88us/step - loss: 0.0027 - val_loss: 0.0231\n",
      "Epoch 61/100\n",
      "426/426 [==============================] - 0s 86us/step - loss: 0.0027 - val_loss: 0.0232\n",
      "Epoch 62/100\n",
      "426/426 [==============================] - 0s 89us/step - loss: 0.0027 - val_loss: 0.0230\n",
      "Epoch 63/100\n",
      "426/426 [==============================] - 0s 87us/step - loss: 0.0027 - val_loss: 0.0229\n",
      "Epoch 64/100\n",
      "426/426 [==============================] - 0s 87us/step - loss: 0.0027 - val_loss: 0.0233\n",
      "Epoch 65/100\n",
      "426/426 [==============================] - 0s 81us/step - loss: 0.0027 - val_loss: 0.0233\n",
      "Epoch 66/100\n",
      "426/426 [==============================] - 0s 82us/step - loss: 0.0027 - val_loss: 0.0232\n",
      "Epoch 67/100\n",
      "426/426 [==============================] - 0s 78us/step - loss: 0.0027 - val_loss: 0.0229\n",
      "Epoch 68/100\n",
      "426/426 [==============================] - 0s 78us/step - loss: 0.0027 - val_loss: 0.0230\n",
      "Epoch 69/100\n",
      "426/426 [==============================] - 0s 102us/step - loss: 0.0027 - val_loss: 0.0233\n",
      "Epoch 70/100\n",
      "426/426 [==============================] - 0s 69us/step - loss: 0.0027 - val_loss: 0.0234\n",
      "Epoch 71/100\n",
      "426/426 [==============================] - 0s 69us/step - loss: 0.0027 - val_loss: 0.0235\n",
      "Epoch 72/100\n",
      "426/426 [==============================] - 0s 77us/step - loss: 0.0027 - val_loss: 0.0232\n",
      "Epoch 73/100\n",
      "426/426 [==============================] - 0s 74us/step - loss: 0.0026 - val_loss: 0.0233\n",
      "Epoch 74/100\n",
      "426/426 [==============================] - 0s 78us/step - loss: 0.0026 - val_loss: 0.0234\n",
      "Epoch 75/100\n",
      "426/426 [==============================] - 0s 91us/step - loss: 0.0026 - val_loss: 0.0231\n",
      "Epoch 76/100\n",
      "426/426 [==============================] - 0s 79us/step - loss: 0.0026 - val_loss: 0.0231\n",
      "Epoch 77/100\n",
      "426/426 [==============================] - 0s 76us/step - loss: 0.0026 - val_loss: 0.0232\n",
      "Epoch 78/100\n",
      "426/426 [==============================] - 0s 77us/step - loss: 0.0026 - val_loss: 0.0232\n",
      "Epoch 79/100\n",
      "426/426 [==============================] - 0s 74us/step - loss: 0.0026 - val_loss: 0.0234\n",
      "Epoch 80/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "426/426 [==============================] - 0s 72us/step - loss: 0.0026 - val_loss: 0.0234\n",
      "Epoch 81/100\n",
      "426/426 [==============================] - 0s 72us/step - loss: 0.0026 - val_loss: 0.0232\n",
      "Epoch 82/100\n",
      "426/426 [==============================] - 0s 80us/step - loss: 0.0026 - val_loss: 0.0233\n",
      "Epoch 83/100\n",
      "426/426 [==============================] - 0s 69us/step - loss: 0.0026 - val_loss: 0.0235\n",
      "Epoch 84/100\n",
      "426/426 [==============================] - 0s 108us/step - loss: 0.0026 - val_loss: 0.0233\n",
      "Epoch 85/100\n",
      "426/426 [==============================] - 0s 93us/step - loss: 0.0026 - val_loss: 0.0233\n",
      "Epoch 86/100\n",
      "426/426 [==============================] - 0s 106us/step - loss: 0.0026 - val_loss: 0.0235\n",
      "Epoch 87/100\n",
      "426/426 [==============================] - 0s 121us/step - loss: 0.0026 - val_loss: 0.0237\n",
      "Epoch 88/100\n",
      "426/426 [==============================] - 0s 134us/step - loss: 0.0026 - val_loss: 0.0238\n",
      "Epoch 89/100\n",
      "426/426 [==============================] - 0s 99us/step - loss: 0.0026 - val_loss: 0.0236\n",
      "Epoch 90/100\n",
      "426/426 [==============================] - 0s 92us/step - loss: 0.0026 - val_loss: 0.0238\n",
      "Epoch 91/100\n",
      "426/426 [==============================] - 0s 80us/step - loss: 0.0026 - val_loss: 0.0239\n",
      "Epoch 92/100\n",
      "426/426 [==============================] - 0s 76us/step - loss: 0.0026 - val_loss: 0.0239\n",
      "Epoch 93/100\n",
      "426/426 [==============================] - 0s 69us/step - loss: 0.0026 - val_loss: 0.0238\n",
      "Epoch 94/100\n",
      "426/426 [==============================] - 0s 64us/step - loss: 0.0026 - val_loss: 0.0237\n",
      "Epoch 95/100\n",
      "426/426 [==============================] - 0s 71us/step - loss: 0.0026 - val_loss: 0.0236\n",
      "Epoch 96/100\n",
      "426/426 [==============================] - 0s 70us/step - loss: 0.0026 - val_loss: 0.0238\n",
      "Epoch 97/100\n",
      "426/426 [==============================] - 0s 76us/step - loss: 0.0026 - val_loss: 0.0238\n",
      "Epoch 98/100\n",
      "426/426 [==============================] - 0s 87us/step - loss: 0.0026 - val_loss: 0.0241\n",
      "Epoch 99/100\n",
      "426/426 [==============================] - 0s 100us/step - loss: 0.0026 - val_loss: 0.0242\n",
      "Epoch 100/100\n",
      "426/426 [==============================] - 0s 102us/step - loss: 0.0026 - val_loss: 0.0241\n"
     ]
    }
   ],
   "source": [
    "# Storing that fit as a history log\n",
    "\n",
    "history_log = model.fit(X_train_s, y_train, \n",
    "          validation_data = (X_test_s, y_test), epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plotting our losses\n",
    "\n",
    "train_loss = history_log.history['loss']\n",
    "test_loss = history_log.history['val_loss']\n",
    "\n",
    "plt.plot(train_loss, label = 'Training Loss')\n",
    "plt.plot(test_loss, label = 'Testing Loss')\n",
    "\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.engine.sequential.Sequential at 0x1a27fff828>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 8.1122059e-01],\n",
       "       [-6.5940738e-02],\n",
       "       [ 6.5286160e-03],\n",
       "       [ 1.1167794e+00],\n",
       "       [ 1.0527958e+00],\n",
       "       [ 1.1017278e-02],\n",
       "       [-1.2869556e-01],\n",
       "       [ 3.6119211e-01],\n",
       "       [ 3.6205959e-01],\n",
       "       [ 8.9337200e-01],\n",
       "       [ 7.5345027e-01],\n",
       "       [ 1.8943335e-01],\n",
       "       [ 9.9714112e-01],\n",
       "       [ 4.1496080e-01],\n",
       "       [ 1.0519683e+00],\n",
       "       [ 1.8509902e-01],\n",
       "       [ 1.0330411e+00],\n",
       "       [ 1.2033768e+00],\n",
       "       [ 7.6311010e-01],\n",
       "       [ 6.9122091e-02],\n",
       "       [ 5.9781444e-01],\n",
       "       [ 9.9003321e-01],\n",
       "       [ 1.3000622e-02],\n",
       "       [ 9.4326270e-01],\n",
       "       [ 9.5422477e-01],\n",
       "       [ 4.6568465e-01],\n",
       "       [ 1.1182590e+00],\n",
       "       [ 8.8423872e-01],\n",
       "       [ 1.1467116e+00],\n",
       "       [-2.8825298e-02],\n",
       "       [ 9.8053533e-01],\n",
       "       [ 1.0687771e+00],\n",
       "       [ 7.4095690e-01],\n",
       "       [ 8.7167937e-01],\n",
       "       [ 1.0065386e+00],\n",
       "       [ 9.2904854e-01],\n",
       "       [ 1.6755654e-01],\n",
       "       [ 9.2700094e-01],\n",
       "       [-2.2698656e-02],\n",
       "       [ 1.0361964e+00],\n",
       "       [ 1.1131722e+00],\n",
       "       [ 3.0809641e-02],\n",
       "       [ 1.0577152e+00],\n",
       "       [ 1.0578548e+00],\n",
       "       [ 2.9299849e-01],\n",
       "       [ 7.4915898e-01],\n",
       "       [ 1.1570195e+00],\n",
       "       [ 1.0548747e+00],\n",
       "       [ 7.3774028e-01],\n",
       "       [ 1.1626168e+00],\n",
       "       [-1.7082615e-01],\n",
       "       [-1.0211711e-01],\n",
       "       [ 4.0547305e-01],\n",
       "       [ 6.3783419e-01],\n",
       "       [ 9.2949456e-01],\n",
       "       [ 8.6290938e-01],\n",
       "       [ 7.8281701e-01],\n",
       "       [-1.8639322e-01],\n",
       "       [ 4.0754795e-01],\n",
       "       [ 1.1383986e+00],\n",
       "       [ 1.1398480e+00],\n",
       "       [-2.0467214e-01],\n",
       "       [-3.7472856e-01],\n",
       "       [ 8.0727226e-01],\n",
       "       [ 1.1007297e+00],\n",
       "       [ 9.4687170e-01],\n",
       "       [ 3.7127987e-02],\n",
       "       [ 1.5879135e-01],\n",
       "       [ 9.4401503e-01],\n",
       "       [ 9.1225487e-01],\n",
       "       [ 2.6210803e-01],\n",
       "       [ 1.0956600e-02],\n",
       "       [ 9.8677909e-01],\n",
       "       [ 1.5268266e-01],\n",
       "       [ 9.1507918e-01],\n",
       "       [ 8.6419779e-01],\n",
       "       [ 1.1098530e+00],\n",
       "       [ 8.1481659e-01],\n",
       "       [ 1.1641812e+00],\n",
       "       [ 6.6430068e-01],\n",
       "       [ 2.2446039e-01],\n",
       "       [ 1.0308590e+00],\n",
       "       [ 4.1213226e-01],\n",
       "       [-2.5013191e-01],\n",
       "       [-1.6037866e-02],\n",
       "       [ 4.0453568e-02],\n",
       "       [-4.2887312e-01],\n",
       "       [ 4.3891862e-02],\n",
       "       [ 1.2117484e+00],\n",
       "       [ 1.1644397e+00],\n",
       "       [ 1.0525007e+00],\n",
       "       [ 6.5532768e-01],\n",
       "       [ 7.6938665e-01],\n",
       "       [ 1.5383146e+00],\n",
       "       [ 1.1115679e+00],\n",
       "       [ 9.2777318e-01],\n",
       "       [ 4.6808496e-02],\n",
       "       [-2.9394025e-01],\n",
       "       [ 1.1386199e+00],\n",
       "       [-8.8404760e-02],\n",
       "       [-7.3316693e-04],\n",
       "       [ 9.5983058e-01],\n",
       "       [ 1.0561587e-01],\n",
       "       [ 2.6733011e-02],\n",
       "       [ 9.6873325e-01],\n",
       "       [ 6.4345384e-01],\n",
       "       [ 1.2315983e+00],\n",
       "       [-1.1458151e-01],\n",
       "       [ 8.8253349e-01],\n",
       "       [ 9.3194556e-01],\n",
       "       [-2.1380381e-01],\n",
       "       [ 1.0242856e+00],\n",
       "       [ 6.4322418e-01],\n",
       "       [-1.4860289e-01],\n",
       "       [ 7.0582539e-01],\n",
       "       [ 3.9702222e-02],\n",
       "       [ 9.1087741e-01],\n",
       "       [ 7.8538775e-01],\n",
       "       [ 7.2249317e-01],\n",
       "       [-9.5701411e-02],\n",
       "       [ 4.8257577e-01],\n",
       "       [ 1.0917199e+00],\n",
       "       [ 9.0455049e-01],\n",
       "       [ 1.9678040e-01],\n",
       "       [ 8.3731914e-01],\n",
       "       [-4.6761259e-02],\n",
       "       [-7.0550933e-02],\n",
       "       [ 1.0302042e+00],\n",
       "       [ 1.0406697e+00],\n",
       "       [ 1.3839953e-01],\n",
       "       [ 2.0197746e-01],\n",
       "       [ 5.0044075e-02],\n",
       "       [ 7.9988509e-01],\n",
       "       [ 1.0049345e+00],\n",
       "       [ 7.5712734e-01],\n",
       "       [ 2.4181423e-01],\n",
       "       [ 4.6274251e-01],\n",
       "       [ 8.8564461e-01],\n",
       "       [ 7.4350017e-01],\n",
       "       [-8.2598642e-02],\n",
       "       [ 9.2614454e-01],\n",
       "       [-1.2253873e-01],\n",
       "       [ 1.0855894e+00]], dtype=float32)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(X_test_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/learn-env/lib/python3.6/site-packages/keras/engine/sequential.py:252: UserWarning: Network returning invalid probability values. The last layer might not normalize predictions into probabilities (like softmax or sigmoid would).\n",
      "  warnings.warn('Network returning invalid probability values. '\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 8.1122059e-01],\n",
       "       [-6.5940738e-02],\n",
       "       [ 6.5286160e-03],\n",
       "       [ 1.1167794e+00],\n",
       "       [ 1.0527958e+00],\n",
       "       [ 1.1017278e-02],\n",
       "       [-1.2869556e-01],\n",
       "       [ 3.6119211e-01],\n",
       "       [ 3.6205959e-01],\n",
       "       [ 8.9337200e-01],\n",
       "       [ 7.5345027e-01],\n",
       "       [ 1.8943335e-01],\n",
       "       [ 9.9714112e-01],\n",
       "       [ 4.1496080e-01],\n",
       "       [ 1.0519683e+00],\n",
       "       [ 1.8509902e-01],\n",
       "       [ 1.0330411e+00],\n",
       "       [ 1.2033768e+00],\n",
       "       [ 7.6311010e-01],\n",
       "       [ 6.9122091e-02],\n",
       "       [ 5.9781444e-01],\n",
       "       [ 9.9003321e-01],\n",
       "       [ 1.3000622e-02],\n",
       "       [ 9.4326270e-01],\n",
       "       [ 9.5422477e-01],\n",
       "       [ 4.6568465e-01],\n",
       "       [ 1.1182590e+00],\n",
       "       [ 8.8423872e-01],\n",
       "       [ 1.1467116e+00],\n",
       "       [-2.8825298e-02],\n",
       "       [ 9.8053533e-01],\n",
       "       [ 1.0687771e+00],\n",
       "       [ 7.4095690e-01],\n",
       "       [ 8.7167937e-01],\n",
       "       [ 1.0065386e+00],\n",
       "       [ 9.2904854e-01],\n",
       "       [ 1.6755654e-01],\n",
       "       [ 9.2700094e-01],\n",
       "       [-2.2698656e-02],\n",
       "       [ 1.0361964e+00],\n",
       "       [ 1.1131722e+00],\n",
       "       [ 3.0809641e-02],\n",
       "       [ 1.0577152e+00],\n",
       "       [ 1.0578548e+00],\n",
       "       [ 2.9299849e-01],\n",
       "       [ 7.4915898e-01],\n",
       "       [ 1.1570195e+00],\n",
       "       [ 1.0548747e+00],\n",
       "       [ 7.3774028e-01],\n",
       "       [ 1.1626168e+00],\n",
       "       [-1.7082615e-01],\n",
       "       [-1.0211711e-01],\n",
       "       [ 4.0547305e-01],\n",
       "       [ 6.3783419e-01],\n",
       "       [ 9.2949456e-01],\n",
       "       [ 8.6290938e-01],\n",
       "       [ 7.8281701e-01],\n",
       "       [-1.8639322e-01],\n",
       "       [ 4.0754795e-01],\n",
       "       [ 1.1383986e+00],\n",
       "       [ 1.1398480e+00],\n",
       "       [-2.0467214e-01],\n",
       "       [-3.7472856e-01],\n",
       "       [ 8.0727226e-01],\n",
       "       [ 1.1007297e+00],\n",
       "       [ 9.4687170e-01],\n",
       "       [ 3.7127987e-02],\n",
       "       [ 1.5879135e-01],\n",
       "       [ 9.4401503e-01],\n",
       "       [ 9.1225487e-01],\n",
       "       [ 2.6210803e-01],\n",
       "       [ 1.0956600e-02],\n",
       "       [ 9.8677909e-01],\n",
       "       [ 1.5268266e-01],\n",
       "       [ 9.1507918e-01],\n",
       "       [ 8.6419779e-01],\n",
       "       [ 1.1098530e+00],\n",
       "       [ 8.1481659e-01],\n",
       "       [ 1.1641812e+00],\n",
       "       [ 6.6430068e-01],\n",
       "       [ 2.2446039e-01],\n",
       "       [ 1.0308590e+00],\n",
       "       [ 4.1213226e-01],\n",
       "       [-2.5013191e-01],\n",
       "       [-1.6037866e-02],\n",
       "       [ 4.0453568e-02],\n",
       "       [-4.2887312e-01],\n",
       "       [ 4.3891862e-02],\n",
       "       [ 1.2117484e+00],\n",
       "       [ 1.1644397e+00],\n",
       "       [ 1.0525007e+00],\n",
       "       [ 6.5532768e-01],\n",
       "       [ 7.6938665e-01],\n",
       "       [ 1.5383146e+00],\n",
       "       [ 1.1115679e+00],\n",
       "       [ 9.2777318e-01],\n",
       "       [ 4.6808496e-02],\n",
       "       [-2.9394025e-01],\n",
       "       [ 1.1386199e+00],\n",
       "       [-8.8404760e-02],\n",
       "       [-7.3316693e-04],\n",
       "       [ 9.5983058e-01],\n",
       "       [ 1.0561587e-01],\n",
       "       [ 2.6733011e-02],\n",
       "       [ 9.6873325e-01],\n",
       "       [ 6.4345384e-01],\n",
       "       [ 1.2315983e+00],\n",
       "       [-1.1458151e-01],\n",
       "       [ 8.8253349e-01],\n",
       "       [ 9.3194556e-01],\n",
       "       [-2.1380381e-01],\n",
       "       [ 1.0242856e+00],\n",
       "       [ 6.4322418e-01],\n",
       "       [-1.4860289e-01],\n",
       "       [ 7.0582539e-01],\n",
       "       [ 3.9702222e-02],\n",
       "       [ 9.1087741e-01],\n",
       "       [ 7.8538775e-01],\n",
       "       [ 7.2249317e-01],\n",
       "       [-9.5701411e-02],\n",
       "       [ 4.8257577e-01],\n",
       "       [ 1.0917199e+00],\n",
       "       [ 9.0455049e-01],\n",
       "       [ 1.9678040e-01],\n",
       "       [ 8.3731914e-01],\n",
       "       [-4.6761259e-02],\n",
       "       [-7.0550933e-02],\n",
       "       [ 1.0302042e+00],\n",
       "       [ 1.0406697e+00],\n",
       "       [ 1.3839953e-01],\n",
       "       [ 2.0197746e-01],\n",
       "       [ 5.0044075e-02],\n",
       "       [ 7.9988509e-01],\n",
       "       [ 1.0049345e+00],\n",
       "       [ 7.5712734e-01],\n",
       "       [ 2.4181423e-01],\n",
       "       [ 4.6274251e-01],\n",
       "       [ 8.8564461e-01],\n",
       "       [ 7.4350017e-01],\n",
       "       [-8.2598642e-02],\n",
       "       [ 9.2614454e-01],\n",
       "       [-1.2253873e-01],\n",
       "       [ 1.0855894e+00]], dtype=float32)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict_proba(X_test_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1]], dtype=int32)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict_classes(X_test_s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = 3\n",
    "b = 5\n",
    "c = a+b\n",
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "a += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- VARIABLE DEPENDENCIES ARE NOT UPDATED IN REAL TIME, C is still 8, BUT HOW DO WE MAKE C = WHATEVER A+B EQUALS WHENEVER WE MAKE A CHANGE TO A or B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensorflow as a graph constructor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constructing the graph\n",
    "\n",
    "a = tf.Variable(3)\n",
    "b = tf.Variable(5)\n",
    "\n",
    "c = a + b\n",
    "d = a + c * b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Starting a session\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    result = sess.run(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'add_1:0' shape=() dtype=int32>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Printing the output\n",
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43\n"
     ]
    }
   ],
   "source": [
    "print(result) #3 + 8 * 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = tf.Variable(4)\n",
    "b = tf.Variable(5)\n",
    "\n",
    "c = a + b\n",
    "d = a + c * b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43\n"
     ]
    }
   ],
   "source": [
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network can also be done in BASE TENSORFLOW (less user friendly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(426, 30)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_s.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "X = tf.placeholder(dtype=tf.float32, shape=(None, 30))\n",
    "y = tf.placeholder(dtype=tf.float32, shape=(None, 1))\n",
    "\n",
    "hid = tf.layers.dense(X, 30, activation=tf.nn.relu)\n",
    "y_hat = tf.layers.dense(hid, 1, activation=tf.nn.sigmoid)\n",
    "\n",
    "loss = tf.losses.log_loss(y, y_hat)\n",
    "optimizer = tf.train.AdamOptimizer(0.01)\n",
    "training_run = optimizer.minimize(loss)\n",
    "\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.986013986013986"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    \n",
    "    for _ in range(100):\n",
    "        sess.run(training_run, feed_dict={X: X_train_s, \n",
    "                                          y: y_train.reshape(-1, 1)})\n",
    "        \n",
    "    pred = sess.run(y_hat, feed_dict={X: X_test_s})\n",
    "\n",
    "classes = (pred > 0.5).astype(int)\n",
    "\n",
    "metrics.accuracy_score(y_test.reshape(-1, 1), classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
